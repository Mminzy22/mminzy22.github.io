---
title: "딥러닝의 발전: 퍼셉트론에서 LLM까지"
author: mminzy22
date: 2025-02-03 20:00:00 +0900
categories: [Machine Learning]
tags: [Bootcamp, Python, Machine Learning, Deep Learning, LLM, TIL]
description: "퍼셉트론부터 LLM까지, 딥러닝의 발전 과정과 핵심 개념"
pin: false
math: true
---


## 1. 가설식: H(x) = wx + b

딥러닝 모델의 기본 개념을 이해하려면, 가장 간단한 선형 모델부터 시작해야 합니다. 가설식은 다음과 같이 표현됩니다:

$$
H(x) = wx + b
$$

- **w**: 입력 신호(자극) x에 대한 가중치. 특정 신호가 더 중요한 경우, 높은 가중치를 가집니다.
- **x**: 입력 데이터(자극값).
- **b**: 편향(bias). 신호의 총합에 추가되어 임계값을 조정하는 역할을 합니다.
- **wx + b**: 모든 신호의 가중치를 곱하고 더한 값으로, 최종적으로 활성화 함수(activation function)를 거쳐 출력이 결정됩니다.

이제 이 개념을 바탕으로 뉴런의 생물학적 구조를 살펴보겠습니다.

## 2. 뉴런(Neuron)이란?

뉴런은 생물학적 신경망의 기본 단위로, 뇌에서 정보를 전달하는 역할을 합니다. 뉴런의 구조와 기능은 인공 신경망(ANN, Artificial Neural Network)의 설계에 영감을 주었습니다.

### 뉴런의 구조
1. **수상돌기(Dendrite)**: 여러 갈래로 뻗어 있는 가지 같은 구조로, 다른 뉴런으로부터 신호를 받습니다.
2. **신경세포체(Cell Body, Soma)**: 뉴런의 본체로 정보를 처리하는 역할을 합니다.
3. **축삭(Axon)**: 신경세포체에서 나온 긴 구조로, 다른 뉴런에게 신호를 전달합니다.
    - 여러 자극값들의 합이 일정 기준을 넘어서면(활성화 함수) 다음 뉴런에게 신호를 전달하며, 기준을 넘지 못하면 신호는 소멸합니다.

이 개념을 기반으로 1950년대에 인공지능 연구자들이 뉴런을 모방한 기계를 만들었습니다. 이것이 바로 **퍼셉트론(Perceptron)**입니다.

## 3. 퍼셉트론(Perceptron): 최초의 인공 뉴런

1958년 프랭크 로젠블랫(Frank Rosenblatt)이 개발한 퍼셉트론은 신경망의 기초가 되는 모델입니다. 퍼셉트론은 단일 뉴런으로 구성되며, 입력값과 가중치의 선형 조합을 기반으로 작동합니다.

### 퍼셉트론의 검증: AND, OR 연산
퍼셉트론이 논리 연산을 수행할 수 있는지 검증하기 위해 **AND**와 **OR** 연산을 실험적으로 확인했습니다.

- **AND 연산**: 두 입력이 모두 1일 때만 1을 출력해야 합니다.
- **OR 연산**: 두 입력 중 하나라도 1이면 1을 출력해야 합니다.

퍼셉트론은 AND, OR 연산을 정확히 수행할 수 있음을 증명하였지만, 한계점도 존재했습니다.

## 4. 퍼셉트론의 한계와 XOR 문제

퍼셉트론이 풀지 못하는 대표적인 문제 중 하나는 **XOR 연산**입니다.

- **XOR 연산**: 두 입력이 다를 때만 1을 출력하는 논리 연산
- 퍼셉트론은 선형 분류기이므로, XOR과 같은 **비선형 문제**를 해결할 수 없습니다.

이 한계로 인해 AI 연구는 한동안 정체기에 빠졌으며, 이를 **AI의 첫 번째 암흑기(AI Winter)**라고 부릅니다.

## 5. 해결책: 다층 퍼셉트론(MLP, Multi-Layer Perceptron)

XOR 문제를 해결하기 위해 **다층 퍼셉트론(MLP)**이 등장했습니다. 다층 퍼셉트론은 하나 이상의 **은닉층(hidden layer)**을 포함하며, 이를 통해 비선형 문제를 해결할 수 있습니다.

### 다층 퍼셉트론의 구조
- **입력층(Input Layer)**: 입력 데이터를 받음
- **은닉층(Hidden Layer)**: 입력 데이터를 변환하여 복잡한 패턴을 학습
- **출력층(Output Layer)**: 최종 결과를 출력

MLP의 도입과 함께 **딥러닝(Deep Learning)**이 시작되었으며, 이후 심층 신경망(DNN, Deep Neural Network)으로 발전하게 됩니다.

## 6. LLM(Large Language Model)과 자연어 처리(NLP)

### LLM이란?
대형 언어 모델(LLM, Large Language Model)은 방대한 데이터를 학습하여 자연어를 이해하고 생성할 수 있는 AI 모델입니다. 대표적인 LLM으로는 **GPT(Generative Pre-trained Transformer), BERT(Bidirectional Encoder Representations from Transformers), LLaMA** 등이 있습니다.

### LLM의 핵심 기술
1. **트랜스포머(Transformer) 아키텍처**: 
   - LLM의 핵심 구조로, 병렬 연산이 가능하여 대규모 데이터 처리에 최적화됨.
   - **셀프 어텐션(Self-Attention)** 메커니즘을 사용하여 문맥을 이해하는 능력을 극대화.
2. **사전 학습(Pre-training)과 미세 조정(Fine-tuning)**:
   - 대량의 텍스트 데이터를 기반으로 사전 학습 후, 특정 작업에 맞게 추가 학습 가능.
3. **거대한 파라미터 수**:
   - 수십억 개 이상의 파라미터를 학습하여 복잡한 언어 이해 및 생성이 가능함.

### LLM의 응용 분야
- **자연어 이해(NLU, Natural Language Understanding)**: 문맥을 파악하고 감정을 분석.
- **텍스트 생성(Text Generation)**: 기사 작성, 소설 창작, 코드 자동 완성.
- **자동 번역(Machine Translation)**: 다양한 언어 간 번역.
- **질의응답 시스템(Q&A Systems)**: 검색 엔진, 챗봇, 가상 비서 등.
- **멀티모달 AI**: 텍스트뿐만 아니라 이미지, 오디오, 비디오 등 다양한 데이터 유형을 이해하고 처리.

### LLM의 한계와 도전 과제
- **연산 비용**: 거대한 모델을 학습하고 실행하는 데 많은 계산 리소스가 필요.
- **데이터 편향(Bias)**: 훈련 데이터의 편향이 모델 결과에 영향을 미칠 수 있음.
- **환각 현상(Hallucination)**: 존재하지 않는 정보를 생성하는 경우가 있음.

### LLM의 미래
최근 LLM은 단순한 텍스트 생성에서 벗어나 **멀티모달 AI**로 발전하고 있으며, 인간과 더욱 자연스럽게 상호작용할 수 있는 AI 시스템으로 진화하고 있습니다.
