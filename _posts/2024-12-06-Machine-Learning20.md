---
title: "도미와 빙어 데이터를 활용한 머신러닝 학습"
author: mminzy22
date: 2024-12-06 10:00:00 +0900
categories: [Machine Learning]
tags: [Bootcamp, Python, Machine Learning, TIL]
description: "도미와 빙어 데이터를 활용해 머신러닝 알고리즘 중 하나인 k-최근접 이웃(KNN)을 학습하고 이를 통해 데이터를 분류하는 방법"
pin: false
---



머신러닝을 처음 시작할 때는 간단한 데이터셋으로 이해하기 쉬운 알고리즘부터 학습하는 것이 좋습니다. 오늘은 **도미와 빙어 데이터**를 활용해 머신러닝 알고리즘 중 하나인 **k-최근접 이웃(KNN)**을 학습하고 이를 통해 데이터를 분류하는 방법을 살펴보겠습니다. 


### 1. 도미 데이터 준비하기

도미 데이터를 **길이(length)**와 **무게(weight)**라는 두 가지 특성으로 표현합니다. 아래는 도미 데이터의 예시입니다.

```python
bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 
                31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 
                35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0]
bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 
                500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 
                700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0]
```


### 2. 도미 데이터 시각화

도미 데이터를 **산점도(scatter plot)**로 시각화하여 길이와 무게 간의 관계를 확인합니다. 이 과정은 데이터를 이해하고, 머신러닝 모델을 적용하기 전의 기본적인 탐색 단계를 수행하기 위함입니다.

```python
import matplotlib.pyplot as plt

# 도미 데이터를 산점도로 시각화
plt.scatter(bream_length, bream_weight, label="Bream", color='blue')
plt.xlabel('Length (cm)')  # x축 레이블: 길이 (cm)
plt.ylabel('Weight (g)')   # y축 레이블: 무게 (g)
plt.title('Bream Data')    # 그래프 제목
plt.legend()               # 범례 추가
plt.show()
```


### 결과 및 분석
- **산점도**는 **선형적 관계**를 보여줍니다. 즉, 도미의 길이가 증가할수록 무게도 비례하여 증가하는 경향이 나타납니다.
- 이처럼 데이터 간의 관계를 **시각적으로 확인**하는 것은 머신러닝 문제 해결의 중요한 첫 단계입니다.
![결과 산점도1]({{ site.baseurl }}/assets/images/2024-12-06_산점도1.png)


#### 💡 추가 팁

1. **산점도란?**
   - 데이터를 x축과 y축에 점으로 표현하여 두 변수 간의 관계를 시각화하는 데 사용됩니다.
   - 두 특성(길이와 무게)의 관계를 쉽게 이해할 수 있습니다.

2. **왜 산점도를 사용하나요?**
   - 데이터를 시각적으로 탐색하며 데이터 간의 관계(선형성, 분포, 이상치 등)를 파악할 수 있습니다.
   - 이런 분석은 적합한 머신러닝 모델을 선택하는 데 도움을 줍니다.

3. **Matplotlib 활용 팁**
   - `color` 매개변수를 통해 데이터의 범주를 색으로 구분할 수 있습니다.
   - `marker` 매개변수를 사용하여 점의 모양을 변경할 수 있습니다. 예: `'^'`는 삼각형, `'o'`는 원.

4. **데이터의 선형적 관계**
   - 산점도가 일직선에 가까운 형태를 보이는 경우, 이를 **선형적 관계**라고 합니다.
   - 선형적 관계는 회귀 분석 등 특정 알고리즘이 잘 작동할 수 있는 데이터임을 암시합니다.


### 3. 빙어 데이터 준비 및 시각화

빙어 데이터를 **산점도(scatter plot)**로 시각화하여 길이와 무게 간의 관계를 확인합니다. 이를 통해 도미와 빙어 데이터를 비교하고, 분류의 가능성을 탐색합니다.

```python
import matplotlib.pyplot as plt

# 빙어 데이터
smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]
smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]

# 빙어 데이터를 산점도로 시각화
plt.scatter(smelt_length, smelt_weight, label="Smelt", color='orange')
plt.xlabel('Length (cm)')  # x축 레이블: 길이 (cm)
plt.ylabel('Weight (g)')   # y축 레이블: 무게 (g)
plt.title('Smelt Data')    # 그래프 제목
plt.legend()               # 범례 추가
plt.show()
```


### 결과 및 분석

1. 빙어 데이터는 도미 데이터와 달리 **무게가 길이에 덜 영향을 받는 특성**을 보입니다.
   - 빙어의 길이는 약 10~15cm, 무게는 약 6~20g 범위에 분포합니다.
   - **도미 데이터**에 비해 산점도가 낮은 위치에 집중되어 있습니다.
   
2. 길이와 무게의 **선형적 관계**가 나타나지만, 도미에 비해 변화율이 낮습니다. 이는 빙어의 길이가 증가해도 무게가 급격히 늘어나지 않음을 의미합니다.

![결과 산점도2]({{ site.baseurl }}/assets/images/2024-12-06_산점도2.png)


#### 💡 추가 팁

1. **두 데이터 비교**
   - 도미와 빙어의 데이터 분포를 한 그래프에 시각화하면, 두 데이터를 **비교**하고 **분류 기준**을 설정할 수 있습니다.
   - 아래는 도미와 빙어 데이터를 한 산점도에 시각화한 코드입니다:

   ```python
   # 도미 데이터
   bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 
                   31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 
                   35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0]
   bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 
                   475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 
                   575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 
                   920.0, 955.0, 925.0, 975.0, 950.0]

   # 도미와 빙어 데이터를 함께 시각화
   plt.scatter(bream_length, bream_weight, label="Bream", color='blue')
   plt.scatter(smelt_length, smelt_weight, label="Smelt", color='orange')
   plt.xlabel('Length (cm)')
   plt.ylabel('Weight (g)')
   plt.title('Bream vs Smelt Data')
   plt.legend()
   plt.show()
   ```

   결과적으로 두 데이터가 서로 **명확히 다른 분포**를 보임을 확인할 수 있습니다.

2. **Matplotlib의 유용한 설정**
   - `alpha` 매개변수를 사용하여 점의 투명도를 설정할 수 있습니다.
   - 예: `plt.scatter(smelt_length, smelt_weight, alpha=0.7)`

3. **머신러닝 적용 준비**
   - 도미와 빙어의 데이터를 결합하여 **특성(feature)**과 **타깃(target)**을 구성하면 머신러닝 알고리즘에 적용할 수 있습니다.


### 4. 머신러닝 알고리즘 적용

이제 준비된 도미와 빙어 데이터를 활용하여 **머신러닝 알고리즘**을 적용합니다. 여기서는 **k-최근접 이웃(K-Nearest Neighbors, KNN)** 알고리즘을 사용해 도미와 빙어를 분류하는 방법을 다룹니다.


#### 1. 데이터 결합: 특성과 타깃 생성

머신러닝 알고리즘에 데이터를 적용하기 위해 도미와 빙어의 길이와 무게를 하나로 합친 2차원 리스트(특성 데이터)를 생성합니다.

```python
# 특성 데이터 생성
length = bream_length + smelt_length
weight = bream_weight + smelt_weight

# zip() 함수로 각 길이와 무게를 하나의 리스트로 묶음
fish_data = [[l, w] for l, w in zip(length, weight)]
```

또한, 각 샘플의 **타깃 데이터**를 생성합니다. 여기서 도미는 `1`, 빙어는 `0`으로 지정합니다.

```python
# 타깃 데이터 생성
fish_target = [1] * len(bream_length) + [0] * len(smelt_length)
```


#### 2. K-최근접 이웃 알고리즘 학습

KNN 알고리즘은 사이킷런의 **`KNeighborsClassifier`** 클래스를 통해 간단히 구현할 수 있습니다.

```python
from sklearn.neighbors import KNeighborsClassifier

# K-최근접 이웃 모델 객체 생성
kn = KNeighborsClassifier(n_neighbors=5)  # 기본값: 5

# 모델 훈련 (fit)
kn.fit(fish_data, fish_target)
```

- `fit()` 메서드는 머신러닝 모델을 훈련시키는 역할을 합니다.
- 이 과정에서 알고리즘은 데이터를 저장하며, 이후 예측 시 데이터를 활용해 가까운 이웃을 찾습니다.


#### 3. 모델 평가

훈련된 모델의 성능을 확인하기 위해 **정확도(accuracy)**를 측정합니다.

```python
# 모델 평가 (score)
accuracy = kn.score(fish_data, fish_target)
print(f"Accuracy: {accuracy:.2f}")
```

- `score()` 메서드는 정확도를 반환합니다. 값이 `1.0`이면 모든 데이터를 정확히 분류했다는 의미입니다.


#### 4. 새로운 데이터 예측

새로운 샘플의 길이와 무게를 입력하여 예측 결과를 확인합니다.

```python
# 새로운 샘플 데이터 예측
new_sample = [[30, 600]]  # 길이 30cm, 무게 600g
prediction = kn.predict(new_sample)
print("Prediction:", "Bream" if prediction[0] == 1 else "Smelt")
```


#### 💡 추가 팁 및 설명

1. **KNN 알고리즘의 특징**
   - 데이터를 학습하는 것이 아니라 저장하는 알고리즘입니다.
   - 새로운 데이터를 입력받으면 저장된 데이터와의 거리를 계산해 가장 가까운 이웃으로 분류합니다.
   - **n_neighbors** 매개변수는 이웃의 개수를 설정합니다. 기본값은 `5`입니다.

2. **성능 개선**
   - KNN은 데이터가 많을수록 계산량이 증가하므로, 데이터 전처리와 최적의 **n_neighbors** 설정이 중요합니다.
   - 적절한 이웃의 개수를 찾으려면 반복적으로 실험하며 정확도를 확인하세요.

   ```python
   # 최적의 n_neighbors 찾기
   for n in range(1, 20):
       kn.n_neighbors = n
       score = kn.score(fish_data, fish_target)
       print(f"{n} neighbors: Accuracy = {score:.2f}")
   ```

3. **머신러닝 모델 평가**
   - **훈련 데이터로만 평가하지 마세요.**
   - 훈련 데이터와 별도로 준비된 테스트 데이터를 사용해야 과적합(overfitting)을 방지할 수 있습니다.

4. **k-최근접 이웃의 단점**
   - 데이터가 많아질수록 연산량이 증가합니다.
   - 데이터 스케일에 민감하므로 전처리가 필요합니다.


#### 🎯 코드 전체 예제

```python
from sklearn.neighbors import KNeighborsClassifier

# 데이터 결합
length = bream_length + smelt_length
weight = bream_weight + smelt_weight
fish_data = [[l, w] for l, w in zip(length, weight)]
fish_target = [1] * len(bream_length) + [0] * len(smelt_length)

# 모델 생성 및 훈련
kn = KNeighborsClassifier(n_neighbors=5)
kn.fit(fish_data, fish_target)

# 모델 평가
accuracy = kn.score(fish_data, fish_target)
print(f"Accuracy: {accuracy:.2f}")

# 새로운 샘플 예측
new_sample = [[30, 600]]
prediction = kn.predict(new_sample)
print("Prediction:", "Bream" if prediction[0] == 1 else "Smelt")
```


### 5. 새로운 데이터 예측

훈련된 **k-최근접 이웃(KNN) 모델**을 사용해 새로운 데이터를 예측합니다. 예측 과정은 모델이 훈련 중 저장한 데이터를 참고하여 새로운 데이터의 클래스를 분류합니다.


#### 1. 새로운 샘플 데이터 입력

길이가 30cm이고 무게가 600g인 샘플 데이터를 예측해봅니다.

```python
# 새로운 샘플 데이터
new_sample = [[30, 600]]

# 예측 결과 확인
prediction = kn.predict(new_sample)
print("Prediction:", "Bream" if prediction[0] == 1 else "Smelt")
```

#### 2. 결과 분석
- 모델은 새로운 샘플을 **Bream(도미)**로 예측합니다. 
- 이는 새로운 데이터가 훈련 데이터에 저장된 도미와 더 가까운 특성을 가지고 있기 때문입니다.


#### 3. 시각화: 새로운 샘플 위치 확인

새로운 샘플을 산점도에 추가하여 기존 데이터와의 상대적 위치를 시각적으로 확인합니다.

```python
import matplotlib.pyplot as plt

# 기존 데이터 시각화
plt.scatter(length[:len(bream_length)], weight[:len(bream_weight)], label="Bream", color='blue')
plt.scatter(length[len(bream_length):], weight[len(bream_weight):], label="Smelt", color='orange')

# 새로운 데이터 추가
plt.scatter(new_sample[0][0], new_sample[0][1], marker='^', label="New Sample", color='red', s=100)

# 그래프 설정
plt.xlabel("Length (cm)")
plt.ylabel("Weight (g)")
plt.legend()
plt.title("Bream and Smelt with New Sample")
plt.show()
```

![결과 산점도3]({{ site.baseurl }}/assets/images/2024-12-06_산점도3.png)


#### 💡 추가 팁
1. **왜 예측이 중요한가?**
   - 새로운 데이터의 예측은 머신러닝 모델의 주요 목표 중 하나입니다.
   - 이 과정은 훈련 데이터와 비교하여 패턴을 찾고, 새로운 샘플을 분류하는 데 사용됩니다.

2. **왜 시각화가 중요한가?**
   - 시각화를 통해 모델의 예측 결과가 직관적으로 이해됩니다.
   - 특히, 새로운 샘플이 기존 데이터에서 어느 위치에 있는지 확인하면 분류의 이유를 파악하기 쉽습니다.


### 6. 추가 도전 과제

KNN 알고리즘의 특성을 이해하기 위해 아래와 같은 추가 실습 과제를 수행해보세요.

#### 1. 이웃의 개수 변경
- **이웃의 개수(n_neighbors)**를 변경하며 모델의 예측 성능을 평가해보세요.

```python
# 이웃의 개수 변경 실험
for n in range(1, 20):
    kn.n_neighbors = n
    accuracy = kn.score(fish_data, fish_target)
    print(f"{n} Neighbors: Accuracy = {accuracy:.2f}")
```

- **Tip**: 이웃의 개수가 너무 크거나 작으면 모델의 성능이 떨어질 수 있습니다.

#### 2. 이상한 샘플 예측
- 기존 데이터와 동떨어진 샘플을 추가하고 모델이 이를 어떻게 예측하는지 확인하세요.

```python
# 이상한 샘플 예측
strange_sample = [[50, 10]]  # 길이 50cm, 무게 10g (비현실적인 조합)
prediction = kn.predict(strange_sample)
print("Prediction for strange sample:", "Bream" if prediction[0] == 1 else "Smelt")
```

- **Tip**: KNN은 단순히 거리를 기준으로 예측하기 때문에 이상한 샘플에도 동작합니다. 이런 상황에서는 추가적인 데이터 전처리가 필요합니다.

#### 3. 훈련 데이터와 테스트 데이터 분리
- 지금까지는 전체 데이터를 훈련과 테스트에 사용했지만, 데이터의 일부를 테스트 세트로 분리하여 모델의 **일반화 성능**을 평가해보세요.

```python
from sklearn.model_selection import train_test_split

# 데이터 분리
train_data, test_data, train_target, test_target = train_test_split(fish_data, fish_target, random_state=42)

# 모델 재훈련 및 평가
kn.fit(train_data, train_target)
test_accuracy = kn.score(test_data, test_target)
print(f"Test Accuracy: {test_accuracy:.2f}")
```


#### 🌟 요약 및 학습 포인트

1. **새로운 데이터 예측**
   - KNN 알고리즘은 훈련 데이터를 기반으로 새로운 데이터를 분류합니다.
   - 예측 결과는 데이터 간 거리와 가장 가까운 이웃의 다수결에 의해 결정됩니다.

2. **추가 실험**
   - 다양한 이웃의 개수와 이상한 샘플을 추가하여 모델의 한계를 탐구하세요.
   - 훈련 데이터와 테스트 데이터를 분리하여 모델의 일반화 성능을 평가하세요.

3. **머신러닝의 목표**
   - 데이터로부터 학습하고, 학습한 내용을 새로운 데이터에 적용하여 유의미한 예측을 수행합니다.


### 7. 정리

머신러닝 알고리즘의 기초를 다루며, 도미와 빙어 데이터를 이용해 **k-최근접 이웃(KNN) 알고리즘**을 학습하고 평가하는 과정을 수행했습니다. 아래는 이번 학습의 주요 내용을 요약한 것입니다.


#### 1. 데이터 준비와 시각화
- **도미와 빙어 데이터**를 리스트로 구성하여 특성 데이터와 타깃 데이터를 생성했습니다.
- 산점도를 통해 각 데이터의 분포와 관계를 시각적으로 확인하며, 데이터 간 선형적 관계를 이해했습니다.

> **팁**: 데이터의 시각화는 머신러닝 모델의 성능을 예측하고 올바른 알고리즘을 선택하는 데 중요한 역할을 합니다.


#### 2. 머신러닝 알고리즘 적용
- **K-최근접 이웃 알고리즘**을 사용해 도미와 빙어 데이터를 분류했습니다.
- 모델 훈련(`fit`), 평가(`score`), 그리고 예측(`predict`)의 기본 흐름을 배웠습니다.
- 새로운 데이터를 입력하여 예측 결과를 확인하고, 산점도를 통해 데이터의 분포를 분석했습니다.


#### 3. K-최근접 이웃 알고리즘의 특징
- **장점**:
  - 단순하지만 효과적이며, 데이터의 패턴을 쉽게 이해할 수 있습니다.
  - 적은 양의 데이터로도 잘 작동합니다.
  
- **단점**:
  - 데이터가 많아질수록 계산량이 증가하여 속도가 느려질 수 있습니다.
  - 데이터의 **스케일**에 민감하므로, 전처리(예: 표준화)가 필요합니다.

> **팁**: 거리 기반 알고리즘인 KNN은 데이터의 스케일 조정이 필수적입니다. 표준점수(z-score)와 같은 스케일링 기법을 적용하세요.


#### 4. 추가 실험과 도전 과제
- 이웃의 개수를 변경하며 모델의 성능 변화를 확인했습니다.
- 이상치나 비현실적인 샘플에 대해 모델이 어떻게 작동하는지 탐구했습니다.
- 훈련 데이터와 테스트 데이터를 분리하여 모델의 일반화 성능을 평가했습니다.


#### 5. 머신러닝 실습 팁
- **올바른 데이터 전처리**:
  - 데이터의 스케일 조정, 이상치 처리, 데이터 분포 확인은 머신러닝의 성능을 결정하는 중요한 요소입니다.
  
- **적절한 모델 선택**:
  - 문제의 유형(회귀, 분류, 클러스터링 등)에 따라 적합한 알고리즘을 선택하세요.
  
- **반복 실험**:
  - 다양한 매개변수를 실험하며 모델 성능을 평가하고, 최적의 설정을 찾아야 합니다.


#### 🌟 핵심 학습 포인트
1. **특성**:
   - 데이터를 표현하는 성질(길이와 무게 등)을 의미합니다.
   - 특성을 통해 머신러닝 모델이 데이터를 이해하고 학습합니다.

2. **훈련**:
   - 머신러닝 모델이 데이터를 학습하는 과정입니다. 사이킷런의 `fit()` 메서드를 통해 수행됩니다.

3. **정확도(Accuracy)**:
   - 머신러닝 모델의 성능을 평가하는 주요 지표입니다.
   - 정확도 = (올바르게 예측한 샘플의 개수) / (전체 샘플의 개수).

4. **모델 평가**:
   - 테스트 데이터를 사용해 모델의 성능을 평가합니다.
   - 훈련 데이터만으로 평가하면 과적합(overfitting)이 발생할 수 있습니다.


### 8. 핵심 패키지와 함수

머신러닝 실습에서 사용된 주요 패키지와 함수들에 대해 간단히 정리합니다. 각 패키지는 데이터 전처리, 시각화, 모델 학습 및 평가에 중요한 역할을 합니다.


#### 1. **Matplotlib**
- **설명**: 데이터 시각화를 위한 파이썬의 대표적인 라이브러리입니다.
- **주요 함수**:
  1. `scatter(x, y, label, color, marker, s)`:
     - **산점도**를 그립니다.
     - `label`: 데이터의 범례를 설정합니다.
     - `color`: 점의 색상을 지정합니다.
     - `marker`: 점의 모양을 지정합니다. (`'o'`, `'^'`, `'D'` 등)
     - `s`: 점의 크기를 설정합니다.
  2. `xlabel(label)` / `ylabel(label)`:
     - x축과 y축의 레이블을 설정합니다.
  3. `title(title)`:
     - 그래프의 제목을 설정합니다.
  4. `legend()`:
     - 그래프에 범례를 추가합니다.
  5. `show()`:
     - 작성된 그래프를 출력합니다.

> **팁**: `alpha` 매개변수로 점의 투명도를 설정해 겹치는 데이터도 명확히 볼 수 있습니다.


#### 2. **NumPy**
- **설명**: 고성능 다차원 배열 연산을 제공하는 파이썬 라이브러리입니다.
- **주요 함수**:
  1. `array(list)`:
     - 리스트를 배열로 변환합니다.
  2. `column_stack(tuples)`:
     - 여러 리스트를 열 방향으로 쌓아 2차원 배열을 생성합니다.
     ```python
     np.column_stack(([1, 2, 3], [4, 5, 6]))
     # 결과: array([[1, 4],
     #               [2, 5],
     #               [3, 6]])
     ```
  3. `concatenate(arrays)`:
     - 여러 배열을 하나로 연결합니다.
     ```python
     np.concatenate(([1, 2], [3, 4]))
     # 결과: array([1, 2, 3, 4])
     ```
  4. `mean(array, axis=0)`:
     - 평균을 계산합니다. `axis=0`은 열 방향 평균을 계산합니다.
  5. `std(array, axis=0)`:
     - 표준편차를 계산합니다.

> **팁**: 데이터 전처리(스케일 조정, 표준점수 계산 등)에서 매우 유용하게 사용됩니다.


#### 3. **Scikit-learn**
- **설명**: 머신러닝 모델을 구현하고 학습시키는 파이썬 라이브러리입니다.
- **주요 클래스 및 메서드**:
  1. **KNeighborsClassifier**:
     - K-최근접 이웃 알고리즘을 구현한 클래스입니다.
     - **생성자 매개변수**:
       - `n_neighbors`: 이웃의 개수를 지정합니다. 기본값은 5입니다.
       - `p`: 거리 계산 방식을 지정합니다. 1은 맨해튼 거리, 2는 유클리드 거리입니다.
       - `n_jobs`: CPU 코어를 지정합니다. -1로 설정하면 모든 코어를 사용합니다.
     - **주요 메서드**:
       1. `fit(X, y)`:
          - 모델을 훈련시킵니다. `X`는 특성 데이터, `y`는 타깃 데이터입니다.
       2. `score(X, y)`:
          - 모델의 성능을 평가하여 정확도를 반환합니다.
       3. `predict(X)`:
          - 새로운 데이터를 입력받아 예측 결과를 반환합니다.
       4. `kneighbors(X)`:
          - 입력 데이터와 가장 가까운 이웃의 거리와 인덱스를 반환합니다.
          ```python
          distances, indexes = kn.kneighbors([[25, 150]])
          ```
  2. **train_test_split**:
     - 데이터를 훈련 세트와 테스트 세트로 나누는 함수입니다.
     - **매개변수**:
       - `test_size`: 테스트 세트의 비율을 설정합니다. 기본값은 0.25(25%).
       - `random_state`: 무작위로 데이터를 섞을 때 사용하는 시드 값입니다.
       - `stratify`: 타깃 데이터를 기준으로 데이터 비율을 유지합니다.
     - **사용 예시**:
       ```python
       train_input, test_input, train_target, test_target = train_test_split(
           fish_data, fish_target, test_size=0.25, random_state=42
       )
       ```

> **팁**: Scikit-learn은 간단한 API로 복잡한 머신러닝 알고리즘을 구현할 수 있습니다. 공식 문서를 참고해 다양한 모델을 탐구해보세요.


#### 🌟 활용 팁

1. **Matplotlib**는 데이터를 탐색하고 관계를 이해하는 데 필수적입니다.
2. **NumPy**는 데이터 전처리 및 계산 효율성을 높이는 데 필수입니다.
3. **Scikit-learn**은 머신러닝 모델을 구현, 평가, 그리고 최적화하는 데 강력한 도구입니다.
