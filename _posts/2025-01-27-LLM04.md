---
title: "Meta의 Llama-2-7b를 On-Premise 환경에서 실행하기"
author: mminzy22
date: 2025-01-27 12:00:00 +0900
categories: [Machine Learning, Deep Learning, LLM]
tags: [Bootcamp, Python, Machine Learning, Deep Learning, LLM, TIL]
description: "Meta의 Llama-2-7b 모델을 On-premise 환경에서 실행하는 방법"
pin: false
---


### 1. Meta의 Llama-2-7b란?
Llama-2-7b는 Meta(구 Facebook)에서 개발한 대형 언어 모델(LLM)로, 다음과 같은 특징을 가지고 있습니다:

- **7b**: 모델의 매개변수(parameter) 수가 70억 개임을 의미하며, 이는 중간 크기 모델로 높은 성능과 비교적 적은 리소스를 요구합니다.
- **대화 모델(Chat Model)**: 특히 사용자와의 대화 응답을 위한 최적화가 이루어진 버전입니다.
- **Hugging Face 지원**: Hugging Face에서 쉽게 사용할 수 있도록 `transformers` 라이브러리를 지원합니다.
- **다양한 사용 사례**: 텍스트 생성, 문서 요약, 질의응답 등 자연어 처리(NLP) 작업에 활용할 수 있습니다.

Llama-2-7b는 OpenAI의 GPT 시리즈와 유사하게 동작하지만, Meta에서 자체적으로 제공하는 모델이며, 상용 및 연구 목적 모두에 사용 가능합니다.


### 2. On-premise란?
**On-premise**는 소프트웨어나 서비스를 사용자의 로컬 환경(사내 서버나 개인 서버)에 설치하고 실행하는 방식입니다. 이는 클라우드 기반 서비스와 대조되는 개념으로, 다음과 같은 주요 특징이 있습니다:

#### 장점:
- **데이터 보안**: 모든 데이터가 로컬 서버에서 처리되므로 데이터 유출 가능성이 줄어듭니다.
- **맞춤형 환경 구성**: 사용자의 요구에 맞게 소프트웨어를 커스터마이징할 수 있습니다.
- **인터넷 의존성 감소**: 인터넷 연결이 불안정하거나 없는 환경에서도 작업이 가능합니다.

#### 단점:
- **초기 비용 부담**: 서버, 스토리지, 네트워크 등 하드웨어 및 인프라 구축 비용이 필요합니다.
- **유지보수 필요**: 소프트웨어 업데이트, 하드웨어 관리, 보안 패치 등을 사용자가 직접 수행해야 합니다.
- **확장성 제한**: 클라우드와 달리 하드웨어 확장이 즉각적이지 않으며, 추가 비용이 발생할 수 있습니다.


### 3. Llama-2-7b를 On-premise에서 실행하기 위한 환경 설정

#### 프로젝트 시작:

```bash
mkdir LLM_practice03
cd LLM_practice03
```

#### 가상환경 생성 및 활성화 (Windows Bash 기준):

```bash
python310 -m venv venv
source venv/Scripts/activate  # Windows Bash
```

#### 필수 패키지 설치:

```bash
pip install transformers huggingface_hub torch
```

#### Hugging Face 계정 생성 및 액세스 토큰 발급:
1. [Hugging Face 홈페이지](https://huggingface.co/)에 접속하여 계정을 생성합니다.
2. 계정 페이지에서 Access Token을 발급받습니다.

#### Hugging Face 로그인:

```bash
huggingface-cli login
```
로그인 시 발급받은 액세스 토큰을 입력합니다.


### 4. Llama-2-7b 실행 스크립트 작성

#### 디렉토리 구성:

```bash
mkdir main
cd main
touch main01.py
```

#### `main01.py` 코드 작성:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

# Llama-2-7b 모델과 토크나이저 불러오기
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf")

# 입력 메시지
input_text = "How is Korea?"
inputs = tokenizer(input_text, return_tensors="pt")

# 텍스트 생성
outputs = model.generate(**inputs, max_length=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```


### 5. 스크립트 실행:

#### 실행 명령어:

```bash
python main01.py
```



### 6. 마무리
On-premise 환경에서 실행하면 데이터 보안과 맞춤형 설정이 가능하다는 장점이 있지만, 하드웨어 관리 및 초기 비용을 고려해야 합니다. Hugging Face의 생태계를 활용하면, 대형 언어 모델을 더욱 간편하게 활용할 수 있습니다.

