---
title: "ML: 데이터 전처리와 K-최근접 이웃 알고리즘 실습"
author: mminzy22
date: 2024-12-09 11:00:00 +0900
categories: [Machine Learning]
tags: [Bootcamp, Python, Machine Learning, TIL]
description: "도미와 빙어 데이터를 이용해 데이터 전처리 과정을 살펴보고, 이를 활용한 K-최근접 이웃(KNN) 알고리즘 실습"
pin: false
---



데이터 전처리는 머신러닝 학습의 중요한 첫 단계로, 데이터의 품질과 알고리즘 성능에 큰 영향을 미칩니다. 오늘은 **도미와 빙어 데이터를 이용해 데이터 전처리 과정**을 살펴보고, 이를 활용한 **K-최근접 이웃(KNN)** 알고리즘을 실습해보겠습니다.


### 1. 데이터 준비 및 병합

먼저, 도미와 빙어 데이터를 길이(`length`)와 무게(`weight`)로 나누어 리스트로 준비합니다.

```python
fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 
               32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 
               35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 
               11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]

fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 
               500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 
               620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 
               975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 
               19.7, 19.9]
```


### 2. NumPy를 활용한 데이터 병합

NumPy 라이브러리를 활용해 데이터를 병합하고 가공하는 과정입니다. 데이터를 머신러닝 알고리즘에서 활용할 수 있도록 준비하는 중요한 단계입니다.

#### 데이터 병합

- **column_stack** 함수를 활용하여 두 개의 리스트(길이와 무게)를 병합합니다. 이는 데이터를 행렬 형태로 변환해 이후 머신러닝 알고리즘에 사용할 수 있는 형태로 준비합니다.

```python
import numpy as np

# NumPy를 사용해 데이터 병합
fish_data = np.column_stack((fish_length, fish_weight))
```

#### 결과 확인

병합한 데이터의 일부를 출력해 데이터가 올바르게 병합되었는지 확인합니다.

```python
# 데이터의 첫 5개 확인
print(fish_data[:5])

# 출력 예시:
# [[ 25.4 242. ]
#  [ 26.3 290. ]
#  [ 26.5 340. ]
#  [ 29.  363. ]
#  [ 29.  430. ]]
```

#### 💡 **추가 팁**
1. **column_stack 함수란?**
   - NumPy에서 제공하는 함수로, 여러 배열을 열(column) 방향으로 합쳐 새로운 2차원 배열을 생성합니다.
   - 배열의 형상이 일치해야 병합할 수 있습니다.

   예시:
   ```python
   np.column_stack(([1, 2, 3], [4, 5, 6]))
   # 결과: [[1, 4],
   #        [2, 5],
   #        [3, 6]]
   ```

2. **데이터 병합 이유**
   - 머신러닝 모델은 데이터를 표 형태로 입력받는 경우가 많습니다. 이를 위해 리스트 형태의 데이터를 2차원 배열로 변환해야 합니다.

3. **병합 데이터 활용**
   - 병합한 데이터는 이후 훈련 데이터로 사용하거나 추가적인 전처리 작업에 활용됩니다.

#### 데이터 병합 이후 작업

이제 데이터를 NumPy 배열로 변환했으므로, **타깃 데이터(fish_target)**도 준비해야 합니다. 이어지는 과정에서 데이터를 훈련 세트와 테스트 세트로 나눌 수 있습니다.


### 3. 타깃 데이터 생성

타깃 데이터는 머신러닝 알고리즘이 학습해야 할 정답(레이블)입니다. 여기서는 **도미**를 `1`로, **빙어**를 `0`으로 설정하여 타깃 데이터를 생성합니다.

#### 타깃 데이터 만들기

NumPy의 **`ones`**와 **`zeros`** 함수를 활용하여 도미와 빙어의 타깃 데이터를 생성합니다.

```python
import numpy as np

# 도미 타깃: 1, 빙어 타깃: 0
fish_target = np.concatenate((np.ones(35), np.zeros(14)))
```

#### 결과 확인

생성한 타깃 데이터를 출력하여 도미와 빙어의 타깃 데이터가 올바르게 생성되었는지 확인합니다.

```python
print(fish_target)

# 출력 예시:
# [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
```

#### 💡 **추가 팁**
1. **`ones`와 `zeros` 함수란?**
   - **`np.ones(n)`**: 길이가 `n`인 배열을 생성하며, 모든 값은 `1`로 채워집니다.
   - **`np.zeros(n)`**: 길이가 `n`인 배열을 생성하며, 모든 값은 `0`으로 채워집니다.

   예시:
   ```python
   np.ones(5)
   # 출력: [1. 1. 1. 1. 1.]
   
   np.zeros(3)
   # 출력: [0. 0. 0.]
   ```

2. **`concatenate` 함수란?**
   - `np.concatenate((a, b))`: 배열 `a`와 `b`를 연결
   - 여러 배열을 하나로 연결해줍니다.
   - 배열을 차례대로 이어붙여 새로운 배열을 만듭니다.

   예시:
   ```python
   np.concatenate((np.ones(3), np.zeros(2)))
   # 출력: [1. 1. 1. 0. 0.]
   ```

3. **타깃 데이터의 역할**
   - 머신러닝 모델은 입력 데이터와 함께 타깃 데이터를 학습합니다.
   - 예측해야 할 데이터의 정답으로 사용됩니다.

#### 타깃 데이터 활용
타깃 데이터는 이후 훈련 데이터와 함께 모델에 입력됩니다. 모델은 이 데이터를 기반으로 도미와 빙어를 구분하는 규칙을 학습하게 됩니다.


### 4. 데이터 분리: 훈련 세트와 테스트 세트

모델을 훈련시키기 위해서는 데이터를 두 부분으로 나누는 것이 중요합니다:
- **훈련 세트**: 모델을 학습시키는 데 사용되는 데이터
- **테스트 세트**: 모델이 훈련되지 않은 데이터에서 얼마나 잘 동작하는지 평가하기 위한 데이터

#### 🛠 데이터 분리를 위한 `train_test_split` 사용

`scikit-learn`(사이킷런)의 **`train_test_split`** 함수를 사용하여 데이터를 자동으로 나눕니다. 이 함수는 데이터를 무작위로 섞어 지정한 비율로 훈련 세트와 테스트 세트로 나눕니다.
- **훈련 세트**: 모델 학습에 사용  
- **테스트 세트**: 모델 평가에 사용  

```python
from sklearn.model_selection import train_test_split

# 데이터 분리 (기본적으로 테스트 세트 비율은 25%)
train_input, test_input, train_target, test_target = train_test_split(
    fish_data, fish_target, random_state=42
)
```

- **`fish_data`**: 입력 데이터 (생선의 길이와 무게)
- **`fish_target`**: 타깃 데이터 (도미: `1`, 빙어: `0`)
- **`random_state`**: 난수 생성 시드 값(재현 가능한 결과를 위해 설정)


#### 결과 확인

데이터가 올바르게 나뉘었는지 `shape` 속성을 사용하여 확인합니다.

```python
# 데이터 크기 출력
print(train_input.shape, test_input.shape)
print(train_target.shape, test_target.shape)

# 출력 예시
# (36, 2) (13, 2)
# (36,) (13,)
```

- **훈련 데이터**: 36개의 샘플 (각 샘플은 2개의 특성을 가짐)
- **테스트 데이터**: 13개의 샘플
- **타깃 데이터**: 1차원 배열


#### 데이터 클래스 확인: 테스트 데이터의 타깃 값

테스트 세트에 도미와 빙어가 고르게 분포되어 있는지 확인합니다.

```python
print(test_target)

# 출력 예시:
# [1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
```


#### 데이터 분리 시 발생할 수 있는 문제: 샘플링 편향

훈련 세트와 테스트 세트의 데이터 비율이 실제 데이터의 클래스 비율과 다를 수 있습니다. 이 현상을 **샘플링 편향**이라고 합니다.

#### 🛠 샘플링 편향 해결: `stratify` 매개변수 사용

`train_test_split` 함수의 **`stratify`** 매개변수를 사용하면 타깃 데이터의 클래스 비율에 맞춰 데이터를 분리할 수 있습니다.

```python
# stratify 매개변수로 클래스 비율을 유지
train_input, test_input, train_target, test_target = train_test_split(
    fish_data, fish_target, stratify=fish_target, random_state=42
)

# 테스트 세트 클래스 비율 확인
print(test_target)

# 출력 예시:
# [0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.]
```


#### 💡 추가 팁

1. **`train_test_split` 주요 매개변수**
   - **`test_size`**: 테스트 세트의 비율. 기본값은 0.25(25%).
   - **`train_size`**: 훈련 세트의 비율. 지정하지 않으면 나머지가 훈련 세트.
   - **`shuffle`**: 데이터를 섞을지 여부. 기본값은 `True`.
   - **`stratify`**: 클래스 비율을 유지. 타깃 데이터를 전달.

2. **클래스 비율 유지가 중요한 이유**
   - 모델 성능을 평가할 때 데이터의 클래스 비율이 실제 분포와 다르면 평가 결과가 왜곡될 수 있습니다.

3. **랜덤 시드 설정**
   - **`random_state`**를 설정하면 데이터가 항상 동일한 방식으로 나뉘어 재현 가능한 결과를 얻을 수 있습니다.


#### 다음 단계
데이터 분리가 완료되었으니, **k-최근접 이웃 알고리즘**을 사용해 모델을 훈련하고 평가하는 단계로 진행할 수 있습니다


### 5. 데이터 전처리: 표준점수로 스케일 조정

**데이터 전처리**는 머신러닝 모델 성능을 향상시키기 위해 데이터를 가공하는 단계입니다. 특히, **거리 기반 알고리즘**(예: k-최근접 이웃)에서는 특성의 스케일(범위)이 서로 다르면 문제가 발생할 수 있습니다.


#### 표준점수란?

**표준점수**(Z-score)는 각 데이터가 평균에서 표준편차의 몇 배만큼 떨어져 있는지를 나타냅니다.  
이를 통해 서로 다른 범위의 특성 값을 동일한 기준으로 비교할 수 있습니다.

- **공식**:  
  \\[
  z = \frac{x - \text{mean}}{\text{std}}
  \\]


#### 평균과 표준편차 계산

먼저, 넘파이를 이용해 훈련 세트의 각 특성에 대해 **평균**과 **표준편차**를 계산합니다.

```python
import numpy as np

# 평균과 표준편차 계산
mean = np.mean(train_input, axis=0)  # 각 열(특성)의 평균
std = np.std(train_input, axis=0)   # 각 열(특성)의 표준편차

print(mean, std)

# 출력 예시:
# [ 27.29722222 454.09722222] [  9.98244253 323.29893931]
```

- **`axis=0`**: 행을 따라 열별로 통계값 계산  
- **`mean`**: 각 열(특성)의 평균  
- **`std`**: 각 열(특성)의 표준편차  


#### 표준점수 변환

훈련 데이터를 표준점수로 변환합니다.  
넘파이의 **브로드캐스팅** 기능을 사용해 각 특성에 평균을 빼고 표준편차로 나눕니다.

```python
# 표준점수 변환
train_scaled = (train_input - mean) / std

print(train_scaled[:5])

# 출력 예시:
# [[-0.95533275 -1.22692606]
#  [ 0.17906907 -0.63224961]
#  [ 0.62304696 -0.13620574]
#  [-0.60710432 -0.96569268]
#  [-1.35074723 -1.25786556]]
```

- 결과는 **스케일이 조정된 훈련 데이터**로, 특성 값들이 평균을 기준으로 표준편차 단위로 조정됩니다.


#### 수상한 도미 샘플 변환

새로운 데이터를 동일한 기준으로 변환해야 합니다.  
**훈련 세트의 평균과 표준편차**를 사용해 변환합니다.

```python
# 새로운 샘플 변환
new_sample = ([25, 150] - mean) / std
print(new_sample)

# 출력 예시:
# [-0.23012309 -0.93987942]
```


#### 변환된 데이터 시각화

변환된 데이터를 산점도로 시각화합니다.

```python
import matplotlib.pyplot as plt

# 훈련 데이터와 새로운 샘플 시각화
plt.scatter(train_scaled[:, 0], train_scaled[:, 1])
plt.scatter(new_sample[0], new_sample[1], marker='^')
plt.xlabel('length (scaled)')
plt.ylabel('weight (scaled)')
plt.show()
```


#### 추가 팁

1. **훈련 세트 기준으로 변환**
   - 반드시 **훈련 세트의 평균과 표준편차**를 사용해 테스트 세트와 새로운 데이터를 변환해야 합니다.
   - 테스트 세트의 평균과 표준편차를 사용하면 데이터 간 비교가 왜곡될 수 있습니다.
   - 테스트 데이터도 훈련 데이터의 평균과 표준편차로 변환해야 합니다.  
      ```python
      test_scaled = (test_input - mean) / std
      ```

2. **데이터 전처리의 중요성**
   - 전처리는 머신러닝에서 필수적인 단계로, 특히 스케일 조정은 거리 기반 알고리즘에서 성능에 직접적인 영향을 미칩니다.

3. **브로드캐스팅** 활용
   - 넘파이의 브로드캐스팅은 크기가 다른 배열 간 연산을 쉽게 수행할 수 있게 해주는 기능입니다. 이를 통해 반복문 없이 효율적으로 연산이 가능합니다.

4. **표준점수 적용 이유**  
   - 거리 기반 알고리즘은 데이터 간의 **유클리드 거리**를 계산합니다.  
   - 특성 간 범위가 다르면 특정 특성이 결과에 영향을 더 많이 줄 수 있으므로, 이를 방지하려면 **표준화**가 필수입니다.


### 6. K-최근접 이웃 모델 학습

표준점수로 변환된 데이터를 활용해 **k-최근접 이웃 모델**을 학습시키고 평가합니다.  
이번 단계에서는 변환된 데이터를 사용해 모델을 다시 훈련하고, **새로운 데이터의 예측**과 **k-최근접 이웃 시각화**를 진행합니다.


#### 모델 학습 및 평가

1. **모델 생성**
   - 사이킷런의 `KNeighborsClassifier` 클래스를 사용합니다.

   ```python
   from sklearn.neighbors import KNeighborsClassifier

   kn = KNeighborsClassifier()  # 모델 객체 생성
   ```

2. **모델 훈련**
   - k-최근접 이웃 모델에 **표준화된 데이터**를 입력해 학습시킵니다.  
   - 사이킷런의 `fit()` 메서드를 사용해 모델을 훈련합니다.
   - 표준화된 훈련 데이터(`train_scaled`)와 타깃 데이터(`train_target`)를 사용하여 학습합니다.
   ```python
   kn.fit(train_scaled, train_target)  # 모델 훈련
   ```

3. **모델 평가**
   - 표준화된 테스트 데이터(`test_scaled`)를 사용하여 성능을 평가합니다.
   
   ```python
   # 테스트 세트 평가
   test_scaled = (test_input - mean) / std
   print(kn.score(test_scaled, test_target))

   # 출력 예시:
   # 1.0
   ```

- **`fit()`**: 모델 학습 메서드
- **`score()`**: 모델 평가 메서드로, 테스트 세트의 **정확도**를 반환합니다.

#### 🧪 **결과 분석**
- **정확도 1.0 (100%)**:  
  테스트 세트에 대해 모든 데이터를 정확히 예측한 것을 의미합니다.

- **K-최근접 이웃(KNN) 알고리즘의 특징**  
   - 새로운 데이터를 예측할 때, 훈련 데이터 중 가장 가까운 `k`개의 데이터를 참고하여 다수결로 분류합니다.

- **적절한 `k`값 설정**  
   - `k`값은 `KNeighborsClassifier` 클래스의 `n_neighbors` 매개변수로 조정 가능합니다.  
   - 기본값은 5이며, 작은 값일수록 훈련 데이터에 민감하게 반응하지만 과적합(overfitting)될 가능성이 있습니다.


#### 새로운 데이터 예측

새로운 샘플 `[25, 150]`을 예측해봅니다.  
이전에 표준점수로 변환한 데이터를 사용합니다.

```python
# 새로운 데이터 예측
print(kn.predict([new_sample]))

# 출력 예시:
# [1.]
```

- 반환된 값 `1.0`은 **도미**를 의미합니다.


#### k-최근접 이웃 탐색

`kneighbors()` 메서드를 사용해 **새로운 샘플의 이웃**을 찾습니다.  
이 메서드는 **거리**와 **이웃 샘플의 인덱스**를 반환합니다.

```python
# k-최근접 이웃 탐색
distances, indexes = kn.kneighbors([new_sample])

print(distances)
print(indexes)

# 출력 예시:
# [[0.28233044 0.49549358 0.68247042 0.73297867 0.81849333]]
# [[16  7  3 14  4]]
```

- **`distances`**: 새로운 샘플과 이웃 샘플 간의 거리
- **`indexes`**: 이웃 샘플의 훈련 세트 인덱스


#### 이웃 시각화

새로운 샘플과 가장 가까운 이웃을 산점도로 확인합니다.  
이웃 샘플은 **마름모 (marker='D')**로 표시합니다.

```python
import matplotlib.pyplot as plt

# 시각화
plt.scatter(train_scaled[:, 0], train_scaled[:, 1])
plt.scatter(new_sample[0], new_sample[1], marker='^', label='New Sample')
plt.scatter(train_scaled[indexes, 0], train_scaled[indexes, 1], marker='D', label='Neighbors')
plt.xlabel('length')
plt.ylabel('weight')
plt.legend()
plt.show()
```


#### 추가 팁

1. **k-최근접 이웃 탐색**
   - `kneighbors()` 메서드로 반환된 이웃 샘플의 거리와 인덱스를 활용해 데이터 분포를 이해할 수 있습니다.

2. **k값 설정**
   - k값(n_neighbors)이 크면 더 많은 데이터를 고려하므로 **일반화 능력**이 증가하지만, 반대로 작은 k값은 모델이 데이터에 과도하게 **적합(overfitting)**할 가능성이 있습니다.
   - 기본값인 `k=5`를 시작점으로 사용하며, 필요에 따라 조정하세요.

3. **새로운 데이터 변환**
   - 항상 훈련 세트의 평균과 표준편차를 기준으로 새로운 데이터를 변환해야 합니다.


### 7. K-최근접 이웃 시각화

이 단계에서는 **k-최근접 이웃 알고리즘**을 시각적으로 이해하기 위해 산점도를 활용합니다.  
모델의 예측 과정과 새로운 샘플이 훈련 데이터와 어떤 관계를 가지는지 확인합니다.


#### 새로운 샘플과 이웃 확인하기

새로운 샘플 `[25, 150]`을 산점도에서 시각화하고, 이 샘플의 **가장 가까운 이웃들**을 확인합니다.

```python
# 새로운 샘플의 이웃 찾기
distances, indexes = kn.kneighbors([new_sample])

# 산점도 시각화
import matplotlib.pyplot as plt

plt.scatter(train_scaled[:, 0], train_scaled[:, 1], label='Train Data')
plt.scatter(new_sample[0], new_sample[1], marker='^', color='r', label='New Sample')
plt.scatter(train_scaled[indexes, 0], train_scaled[indexes, 1], marker='D', color='g', label='Neighbors')
plt.xlabel('Length (standardized)')
plt.ylabel('Weight (standardized)')
plt.legend()
plt.show()
```


#### 그래프 설명

1. **산점도의 각 점**
   - 훈련 데이터는 **파란색 점**으로 표시됩니다.
   - 새로운 샘플은 **빨간색 삼각형 (marker='^')**으로 나타납니다.
   - 이웃 샘플은 **초록색 마름모 (marker='D')**로 강조됩니다.

2. **X축과 Y축**
   - X축: 길이(표준화)
   - Y축: 무게(표준화)

3. **결과 해석**
   - 삼각형으로 표시된 새로운 샘플이 어떤 데이터와 가까운지 시각적으로 확인할 수 있습니다.
   - 이웃 샘플은 다수결 원칙에 따라 **도미(1)**로 예측됩니다.


#### k-최근접 이웃 모델의 동작

k-최근접 이웃 알고리즘은 새로운 데이터를 예측하기 위해 **가장 가까운 k개의 샘플**을 확인합니다.  
샘플 간 거리는 **유클리디안 거리**를 사용하여 계산되며, 가장 가까운 샘플들이 다수결로 예측값을 결정합니다.

#### 유클리디안 거리 계산 공식:
\\[
\text{Distance} = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}
\\]
- \\(x_i, y_i\\): 두 샘플의 각 특성 값
- \\(n\\): 특성의 개수


#### 추가 팁

1. **시각화를 통한 이해**  
   모델의 예측 과정을 시각적으로 확인하면 데이터 간 관계와 모델의 동작 원리를 직관적으로 이해할 수 있습니다.

2. **샘플의 위치 확인**  
   새로운 샘플이 이웃 데이터와 얼마나 가까운지를 확인해 모델이 예측한 결과를 검증할 수 있습니다.

3. **k값 조정**  
   다른 k값을 사용해 동일한 시각화를 반복하면, k값 변화에 따른 예측 결과를 관찰할 수 있습니다.


#### 핵심 개념 정리

이번 과정에서는 **k-최근접 이웃(K-Nearest Neighbors, KNN)** 알고리즘과 **데이터 전처리**의 중요성을 배웠습니다.  
아래는 핵심 개념을 정리한 내용입니다.


#### 🛠️ **핵심 개념**
1. **특성 (Feature)**  
   - 데이터를 설명하는 속성입니다.  
   - 생선 데이터에서는 길이와 무게가 특성입니다.

2. **샘플링 편향**
   - 데이터를 훈련 세트와 테스트 세트로 나눌 때 **stratify 매개변수**를 통해 클래스 비율을 유지.
   - 클래스 간 불균형은 모델 성능에 영향을 미칠 수 있음.

3. **k-최근접 이웃 알고리즘**  
   - 새로운 데이터를 예측할 때 가장 가까운 k개의 데이터를 기반으로 분류합니다.  
   - 데이터의 **스케일 차이**는 모델의 성능에 큰 영향을 미치므로 표준화가 중요.
   - `kneighbors()` 메서드:
      - 주어진 데이터와 가장 가까운 이웃을 찾아 거리를 반환.
      - 가까운 이웃을 시각적으로 확인 가능.

4. **데이터 분리: 훈련 세트와 테스트 세트**  
   - 모델 평가를 위해 데이터를 훈련과 테스트로 나누는 과정이 필요합니다.  
   - 비율은 보통 75:25 또는 80:20으로 나눕니다.

5. **데이터 전처리 (Data Preprocessing)**  
   - 모델 학습 전에 데이터를 정제하고 변환하는 과정입니다.  
   - 표준점수, 원-핫 인코딩, 정규화 등이 포함됩니다.
   - **데이터 전처리**는 머신러닝 모델의 성능을 향상시키기 위해 반드시 필요한 과정입니다.
   - **표준점수(Z-score)**:
      - 특성의 평균을 빼고 표준편차로 나누어 데이터의 스케일을 조정.
      - 서로 다른 범위를 가진 데이터를 균일한 기준으로 비교 가능.
      - 훈련 세트의 평균과 표준편차를 기준으로 테스트 세트에도 동일하게 적용해야 함.
   - **브로드캐스팅(Broadcasting)**:
      - 넘파이 배열 간 크기가 다를 때 자동으로 연산을 확장해 수행.

6. **시각화**  
   - 데이터를 시각화하면 모델의 학습 및 예측 결과를 더 쉽게 이해할 수 있습니다.


### 핵심 패키지와 함수

머신러닝 작업에서 자주 사용하는 **Python 라이브러리**와 이번 실습에서 활용한 함수들을 정리합니다. 📂


#### 🔹 **NumPy**
1. `np.array()`  
   - 파이썬 리스트를 넘파이 배열로 변환합니다.  
     ```python
     np.array([1, 2, 3])
     ```

2. `np.column_stack()`  
   - 여러 리스트를 열 방향으로 병합합니다.
   - 여러 1차원 배열을 열(column)로 묶어 2차원 배열 생성.  
      ```python
      np.column_stack((list1, list2))
      ```

3. `np.ones()` / `np.zeros()`
   - 모든 원소가 1 또는 0인 배열 생성.
      ```python
      ones = np.ones(5)
      zeros = np.zeros(10)
      ```

4. `np.mean()`  
   - 배열의 평균을 계산합니다.  
      ```python
      np.mean(array, axis=0)
      ```

5. `np.std()`  
   - 배열의 표준편차를 계산합니다.  
      ```python
      np.std(array, axis=0)
      ```

6. **브로드캐스팅**  
   - 배열 간 연산에서 자동으로 크기를 맞춰주는 넘파이 기능입니다.  
      ```python
      standardized_array = (array - mean) / std
      ```

7. `np.random.shuffle()`
   - 배열을 무작위로 섞음.
      ```python
      np.random.shuffle(arr)
      ```

#### 🔹 **scikit-learn**
1. `train_test_split()`  
   - 데이터를 훈련 세트와 테스트 세트로 나눕니다.
   - **매개변수:**
    - `test_size`: 테스트 세트의 비율 (기본값: 0.25).
    - `shuffle`: 데이터를 나누기 전에 섞을지 여부 (기본값: `True`).
    - `stratify`: 클래스 비율을 유지하며 데이터를 나눌 때 사용.  
     ```python
     from sklearn.model_selection import train_test_split
     train_input, test_input, train_target, test_target = train_test_split(data, target, stratify=target, random_state=42)
     ```

2. `KNeighborsClassifier()`  
   - k-최근접 이웃 모델을 생성합니다.  
      ```python
      from sklearn.neighbors import KNeighborsClassifier
      kn = KNeighborsClassifier(n_neighbors=5)
      ```

3. `kn.fit()`  
   - 모델을 훈련합니다.  
     ```python
     kn.fit(train_input, train_target)
     ```

4. `kn.score()`  
   - 모델의 성능(정확도)을 평가합니다.  
     ```python
     kn.score(test_input, test_target)
     ```

5. `kn.predict()`  
   - 새로운 데이터를 예측합니다.  
     ```python
     kn.predict([[25, 150]])
     ```

6. `kn.kneighbors()`  
   - 입력 데이터의 가장 가까운 k개의 이웃을 반환합니다.  
     ```python
     distances, indexes = kn.kneighbors([new_sample])
     ```


#### 🔹 **Matplotlib**
1. `plt.scatter()`  
   - 산점도를 그립니다.
   - **매개변수:**
    - `x`, `y`: x축, y축 데이터.
    - `marker`: 마커 스타일 지정 (예: `'o'`, `'^'`, `'D'` 등).
    - `c`: 마커 색상 지정. 
     ```python
     plt.scatter(x_values, y_values, marker='o', color='blue')
     ```

2. `plt.xlabel()` / `plt.ylabel()`  
   - X축과 Y축의 레이블을 설정합니다.  
     ```python
     plt.xlabel('Length')
     plt.ylabel('Weight')
     ```

3. `plt.xlim()` / `plt.ylim()`  
   - X축과 Y축의 범위를 설정합니다.  
     ```python
     # 개별 값 방식
     plt.xlim(0, 100)
     plt.ylim(0, 500)
     
     # 튜플 방식 - 동적 생성
     plt.xlim((0, 100))
     plt.ylim((0, 50))
     ```

4. `plt.legend()`  
   - 범례를 추가합니다.  
     ```python
     plt.legend(['Train Data', 'New Sample'])
     ```

5. `plt.show()`  
   - 그래프를 화면에 출력합니다.  
     ```python
     plt.show()
     ```
